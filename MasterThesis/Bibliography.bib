@article{twinanda2015data,
  title={Data-driven spatio-temporal RGBD feature encoding for action recognition in operating rooms},
  author={Twinanda, Andru P and Alkan, Emre O and Gangi, Afshin and de Mathelin, Michel and Padoy, Nicolas},
  journal={International journal of computer assisted radiology and surgery},
  pages={1--11},
  year={2015},
  publisher={Springer Berlin Heidelberg}
}

@article{Zhu201320,
title = "Multi-view action recognition using local similarity random forests and sensor fusion",
journal = "Pattern Recognition Letters ",
volume = "34",
number = "1",
pages = "20 - 24",
year = "2013",
note = "Extracting Semantics from Multi-Spectrum Video ",
issn = "0167-8655",
doi = "http://dx.doi.org/10.1016/j.patrec.2012.04.016",
url = "http://www.sciencedirect.com/science/article/pii/S0167865512001407",
author = "Fan Zhu and Ling Shao and Mingxiu Lin",
keywords = "Local similarity",
keywords = "Random forests",
keywords = "Sensor fusion",
keywords = "Voting strategy",
keywords = "\{IXMAS\}",
keywords = "Action recognition ",
abstract = "This paper addresses the multi-view action recognition problem with a local segment similarity voting scheme, upon which we build a novel multi-sensor fusion method. The recently proposed random forests classifier is used to map the local segment features to their corresponding prediction histograms. We compare the results of our approach with those of the baseline Bag-of-Words (BoW) and the Naïve–Bayes Nearest Neighbor (NBNN) methods on the multi-view \{IXMAS\} dataset. Additionally, comparisons between our multi-camera fusion strategy and the normally used early feature concatenating strategy are also carried out using different camera views and different segment scales. It is proven that the proposed sensor fusion technique, coupled with the random forests classifier, is effective for multiple view human action recognition. "
}

@inproceedings{Bhatia:2007:RIO:1620113.1620126,
 author = {Bhatia, Beenish and Oates, Tim and Xiao, Yan and Hu, Peter},
 title = {Real-time Identification of Operating Room State from Video},
 booktitle = {Proceedings of the 19th National Conference on Innovative Applications of Artificial Intelligence - Volume 2},
 series = {IAAI'07},
 year = {2007},
 isbn = {978-1-57735-323-2},
 location = {Vancouver, British Columbia, Canada},
 pages = {1761--1766},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1620113.1620126},
 acmid = {1620126},
 publisher = {AAAI Press},
} 

@article{DBLP:journals/tbe/LalysRBJ12,
  author    = {Florent Lalys and
               Laurent Riffaud and
               David Bouget and
               Pierre Jannin},
  title     = {A Framework for the Recognition of High-Level Surgical Tasks From
               Video Images for Cataract Surgeries},
  journal   = {{IEEE} Trans. Biomed. Engineering},
  volume    = {59},
  number    = {4},
  pages     = {966--976},
  year      = {2012},
  url       = {http://dx.doi.org/10.1109/TBME.2011.2181168},
  doi       = {10.1109/TBME.2011.2181168},
  timestamp = {Fri, 23 Mar 2012 08:45:37 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/tbe/LalysRBJ12},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Padoy2012632,
title = "Statistical modeling and recognition of surgical workflow ",
journal = "Medical Image Analysis ",
volume = "16",
number = "3",
pages = "632 - 641",
year = "2012",
note = "Computer Assisted Interventions ",
issn = "1361-8415",
doi = "http://dx.doi.org/10.1016/j.media.2010.10.001",
url = "http://www.sciencedirect.com/science/article/pii/S1361841510001131",
author = "Nicolas Padoy and Tobias Blum and Seyed-Ahmad Ahmadi and Hubertus Feussner and Marie-Odile Berger and Nassir Navab",
keywords = "Surgical workflow",
keywords = "Context aware operating room",
keywords = "Surgical assistance system",
keywords = "Hidden Markov Model",
keywords = "Cholecystectomy ",
abstract = "In this paper, we contribute to the development of context-aware operating rooms by introducing a novel approach to modeling and monitoring the workflow of surgical interventions. We first propose a new representation of interventions in terms of multidimensional time-series formed by synchronized signals acquired over time. We then introduce methods based on Dynamic Time Warping and Hidden Markov Models to analyze and process this data. This results in workflow models combining low-level signals with high-level information such as predefined phases, which can be used to detect actions and trigger an event. Two methods are presented to train these models, using either fully or partially labeled training surgeries. Results are given based on tool usage recordings from sixteen laparoscopic cholecystectomies performed by several surgeons. "
}

@article{Zappella2013732,
title = "Surgical gesture classification from video and kinematic data ",
journal = "Medical Image Analysis ",
volume = "17",
number = "7",
pages = "732 - 745",
year = "2013",
note = "Special Issue on the 2012 Conference on Medical Image Computing and Computer Assisted Intervention ",
issn = "1361-8415",
doi = "http://dx.doi.org/10.1016/j.media.2013.04.007",
url = "http://www.sciencedirect.com/science/article/pii/S1361841513000522",
author = "Luca Zappella and Benjamín Béjar and Gregory Hager and René Vidal",
keywords = "Surgical gesture classification",
keywords = "Time series classification",
keywords = "Dynamical system classification",
keywords = "Bag of features",
keywords = "Multiple kernel learning ",
abstract = "Abstract Much of the existing work on automatic classification of gestures and skill in robotic surgery is based on dynamic cues (e.g., time to completion, speed, forces, torque) or kinematic data (e.g., robot trajectories and velocities). While videos could be equally or more discriminative (e.g., videos contain semantic information not present in kinematic data), they are typically not used because of the difficulties associated with automatic video interpretation. In this paper, we propose several methods for automatic surgical gesture classification from video data. We assume that the video of a surgical task (e.g., suturing) has been segmented into video clips corresponding to a single gesture (e.g., grabbing the needle, passing the needle) and propose three methods to classify the gesture of each video clip. In the first one, we model each video clip as the output of a linear dynamical system (LDS) and use metrics in the space of \{LDSs\} to classify new video clips. In the second one, we use spatio-temporal features extracted from each video clip to learn a dictionary of spatio-temporal words, and use a bag-of-features (BoF) approach to classify new video clips. In the third one, we use multiple kernel learning (MKL) to combine the \{LDS\} and BoF approaches. Since the \{LDS\} approach is also applicable to kinematic data, we also use \{MKL\} to combine both types of data in order to exploit their complementarity. Our experiments on a typical surgical training setup show that methods based on video data perform equally well, if not better, than state-of-the-art approaches based on kinematic data. In turn, the combination of both kinematic and video data outperforms any other algorithm based on one type of data alone. "
}

@INPROCEEDINGS{6553758, 
author={Chakraborty, I. and Elgammal, A. and Burd, R.S.}, 
booktitle={Automatic Face and Gesture Recognition (FG), 2013 10th IEEE International Conference and Workshops on}, 
title={Video based activity recognition in trauma resuscitation}, 
year={2013}, 
month={April}, 
pages={1-8}, 
abstract={We present a system for automated transcription of trauma resuscitation in the emergency department (ED). Using a ceiling-mounted single camera video recording, our goal is to track and transcribe the medical procedures performed during resuscitation of a patient, the time instances of their initiation and their temporal durations. In this multi-agent, multitask setting, we represent procedures as high-level concepts composed of low-level features based on the patient's pose, scene dynamics, clinician motions and device locations. In particular, the low-level features are transformed into intermediate action attributes (e.g., “hand grasping of an object of interest”) and are used as building blocks to describe procedures. Procedures are expressed as first-order logic statements that capture spatio-temporal attribute interactions compactly in an activity grammar. The probabilities from feature observations and the logical semantics are combined probabilistically in a Markov Logic Network (MLN). At runtime, a Markov Network is dynamically constructed representing hypothesized procedures, spatio-temporal relationships and attribute probabilities. Inference on this network determines the most consistent sequence of procedures over time. Our activity model is modular and extendible to a multitude of sensor inputs and detection methods. The method is thus adaptable to many activity recognition problems. In this paper, we show our approach using videos of simulated trauma simulations. The accuracy of the results confirms the suitability of our framework.}, 
keywords={Markov processes;image motion analysis;injuries;medical image processing;multi-agent systems;object recognition;video signal processing;MLN;Markov logic network;action attribute;activity grammar;automated transcription;ceiling-mounted single camera video recording;clinician motion;emergency department;feature observation;first-order logic statement;hand grasping;logical semantics;low-level feature;medical procedure;multiagent;multitask setting;patient pose;scene dynamics;spatio-temporal attribute interaction;temporal duration;trauma resuscitation;video based activity recognition;Biomedical imaging;Feature extraction;Grammar;Hidden Markov models;Markov random fields;Tracking}, 
doi={10.1109/FG.2013.6553758},}

@article{seeingIsBelievingNL,
year={2015},
issn={1861-6410},
journal={International Journal of Computer Assisted Radiology and Surgery},
doi={10.1007/s11548-015-1161-x},
title={Seeing is believing: increasing intraoperative awareness to scattered radiation in interventional procedures by combining augmented reality, Monte Carlo simulations and wireless dosimeters},
url={http://dx.doi.org/10.1007/s11548-015-1161-x},
publisher={Springer Berlin Heidelberg},
keywords={Augmented reality; Radiation monitoring; Surgical workflow analysis; Interventional radiology; Monte Carlo simulations; RGBD cameras},
author={Loy Rodas, Nicolas and Padoy, Nicolas},
pages={1-11},
language={English}
}

@INPROCEEDINGS{1570899, 
author={Dollar, P. and Rabaud, V. and Cottrell, G. and Belongie, S.}, 
booktitle={Visual Surveillance and Performance Evaluation of Tracking and Surveillance, 2005. 2nd Joint IEEE International Workshop on}, 
title={Behavior recognition via sparse spatio-temporal features}, 
year={2005}, 
month={Oct}, 
pages={65-72}, 
abstract={A common trend in object recognition is to detect and leverage the use of sparse, informative feature points. The use of such features makes the problem more manageable while providing increased robustness to noise and pose variation. In this work we develop an extension of these ideas to the spatio-temporal case. For this purpose, we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate, and we propose an alternative. Anchoring off of these interest points, we devise a recognition algorithm based on spatio-temporally windowed data. We present recognition results on a variety of datasets including both human and rodent behavior.}, 
keywords={image recognition;object detection;behavior recognition;object recognition;spatio-temporal features;Computer science;Computer vision;Detectors;Mice;Object detection;Object recognition;Prototypes;Robustness;Spatiotemporal phenomena;Video sequences}, 
doi={10.1109/VSPETS.2005.1570899},}

@INPROCEEDINGS{Laptev03space-timeinterest,
    author = {Ivan Laptev and Tony Lindeberg},
    title = {Space-time Interest Points},
    booktitle = {IN ICCV},
    year = {2003},
    pages = {432--439},
    publisher = {}
}

@article{laptev2005space,
  title={On space-time interest points},
  author={Laptev, Ivan},
  journal={International Journal of Computer Vision},
  volume={64},
  number={2-3},
  pages={107--123},
  year={2005},
  publisher={Kluwer Academic Publishers}
}


@INPROCEEDINGS{6619209, 
author={Lu Xia and Aggarwal, J.K.}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on}, 
title={Spatio-temporal Depth Cuboid Similarity Feature for Activity Recognition Using Depth Camera}, 
year={2013}, 
month={June}, 
pages={2834-2841}, 
abstract={Local spatio-temporal interest points (STIPs) and the resulting features from RGB videos have been proven successful at activity recognition that can handle cluttered backgrounds and partial occlusions. In this paper, we propose its counterpart in depth video and show its efficacy on activity recognition. We present a filtering method to extract STIPs from depth videos (called DSTIP) that effectively suppress the noisy measurements. Further, we build a novel depth cuboid similarity feature (DCSF) to describe the local 3D depth cuboid around the DSTIPs with an adaptable supporting size. We test this feature on activity recognition application using the public MSRAction3D, MSRDailyActivity3D datasets and our own dataset. Experimental evaluation shows that the proposed approach outperforms state-of-the-art activity recognition algorithms on depth videos, and the framework is more widely applicable than existing approaches. We also give detailed comparisons with other features and analysis of choice of parameters as a guidance for applications.}, 
keywords={cameras;feature extraction;filtering theory;image denoising;image recognition;video signal processing;3D depth cuboid;DCSF;DSTIP;MSRDailyActivity3D datasets;RGB videos;STIPs;activity recognition algorithm;cluttered backgrounds;depth camera;depth video;filtering method;local spatiotemporal interest points;noisy measurement suppression;partial occlusions;public MSRAction3D dataset;spatiotemporal depth cuboid similarity feature extraction;Cameras;Detectors;Feature extraction;Histograms;Noise;Three-dimensional displays;Videos;Kinect;Spatio temporal interest point;activity recognition;depth image}, 
doi={10.1109/CVPR.2013.365}, 
ISSN={1063-6919},}

@INPROCEEDINGS{6909503, 
author={Xiaodong Yang and YingLi Tian}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on}, 
title={Super Normal Vector for Activity Recognition Using Depth Sequences}, 
year={2014}, 
month={June}, 
pages={804-811}, 
abstract={This paper presents a new framework for human activity recognition from video sequences captured by a depth camera. We cluster hypersurface normals in a depth sequence to form the polynormal which is used to jointly characterize the local motion and shape information. In order to globally capture the spatial and temporal orders, an adaptive spatio-temporal pyramid is introduced to subdivide a depth video into a set of space-time grids. We then propose a novel scheme of aggregating the low-level polynormals into the super normal vector (SNV) which can be seen as a simplified version of the Fisher kernel representation. In the extensive experiments, we achieve classification results superior to all previous published results on the four public benchmark datasets, i.e., MSRAction3D, MSRDailyActivity3D, MSRGesture3D, and MSRActionPairs3D.}, 
keywords={image sequences;spatiotemporal phenomena;vectors;video signal processing;SNV;adaptive spatio-temporal pyramid;depth sequences;human activity recognition;motion information;shape information;space-time grids;super normal vector;video sequences;Dictionaries;Encoding;Joints;Trajectory;Vectors;Visualization}, 
doi={10.1109/CVPR.2014.108},}

@INPROCEEDINGS{6909735, 
author={Jiang Wang and Xiaohan Nie and Yin Xia and Ying Wu and Song-Chun Zhu}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on}, 
title={Cross-View Action Modeling, Learning, and Recognition}, 
year={2014}, 
month={June}, 
pages={2649-2656}, 
abstract={Existing methods on video-based action recognition are generally view-dependent, i.e., performing recognition from the same views seen in the training data. We present a novel multiview spatio-temporal and-or graph (MST-AOG) representation for cross-view action recognition, i.e., the recognition is performed on the video from an unknown and unseen view. As a compositional model, MST-AOG compactly represents the hierarchical combinatorial structures of cross-view actions by explicitly modeling the geometry, appearance and motion variations. This paper proposes effective methods to learn the structure and parameters of MST-AOG. The inference based on MST-AOG enables action recognition from novel views. The training of MST-AOG takes advantage of the 3D human skeleton data obtained from Kinect cameras to avoid annotating enormous multi-view video frames, which is error-prone and time-consuming, but the recognition does not need 3D information and is based on 2D video input. A new Multiview Action3D dataset has been created and will be released. Extensive experiments have demonstrated that this new action representation significantly improves the accuracy and robustness for cross-view action recognition on 2D videos.}, 
keywords={cameras;geometry;graph theory;image motion analysis;image recognition;image representation;video signal processing;2D videos;3D human skeleton data;Kinect cameras;MST-AOG representation;action representation;appearance modeling;compositional model;cross-view action learning;cross-view action modeling;cross-view action recognition;cross-view actions;geometry modeling;hierarchical combinatorial structures;motion variation modeling;multiview action 3D dataset;multiview spatio-temporal AND-OR graph representation;video-based action recognition;Joints;Pattern recognition;Solid modeling;Three-dimensional displays;Training;Training data}, 
doi={10.1109/CVPR.2014.339},}

@INPROCEEDINGS{6909476, 
author={Vemulapalli, R. and Arrate, F. and Chellappa, R.}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on}, 
title={Human Action Recognition by Representing 3D Skeletons as Points in a Lie Group}, 
year={2014}, 
month={June}, 
pages={588-595}, 
abstract={Recently introduced cost-effective depth sensors coupled with the real-time skeleton estimation algorithm of Shotton et al. [16] have generated a renewed interest in skeleton-based human action recognition. Most of the existing skeleton-based approaches use either the joint locations or the joint angles to represent a human skeleton. In this paper, we propose a new skeletal representation that explicitly models the 3D geometric relationships between various body parts using rotations and translations in 3D space. Since 3D rigid body motions are members of the special Euclidean group SE(3), the proposed skeletal representation lies in the Lie group SE(3)×.. .×SE(3), which is a curved manifold. Using the proposed representation, human actions can be modeled as curves in this Lie group. Since classification of curves in this Lie group is not an easy task, we map the action curves from the Lie group to its Lie algebra, which is a vector space. We then perform classification using a combination of dynamic time warping, Fourier temporal pyramid representation and linear SVM. Experimental results on three action datasets show that the proposed representation performs better than many existing skeletal representations. The proposed approach also outperforms various state-of-the-art skeleton-based human action recognition approaches.}, 
keywords={Fourier analysis;Lie algebras;Lie groups;curve fitting;estimation theory;gesture recognition;image motion analysis;support vector machines;3D geometric relationship;3D rigid body motion;3D skeletons;3D space;Fourier temporal pyramid representation;Lie algebra;Lie group;action curves;cost-effective depth sensors;curved manifold;dynamic time warping;human skeleton;joint angle;joint location;linear SVM;real-time skeleton estimation algorithm;skeletal representation;skeleton-based approach;skeleton-based human action recognition;special Euclidean group;vector space;Algebra;Geometry;Hidden Markov models;Joints;Sensors;Three-dimensional displays;Action Recognition;Lie Groups;Special Euclidean Group}, 
doi={10.1109/CVPR.2014.82},}

@INPROCEEDINGS{6909731, 
author={Yen-Yu Lin and Ju-Hsuan Hua and Tang, N.C. and Min-Hung Chen and Liao, H.-Y.M.}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on}, 
title={Depth and Skeleton Associated Action Recognition without Online Accessible RGB-D Cameras}, 
year={2014}, 
month={June}, 
pages={2617-2624}, 
abstract={The recent advances in RGB-D cameras have allowed us to better solve increasingly complex computer vision tasks. However, modern RGB-D cameras are still restricted by the short effective distances. The limitation may make RGB-D cameras not online accessible in practice, and degrade their applicability. We propose an alternative scenario to address this problem, and illustrate it with the application to action recognition. We use Kinect to offline collect an auxiliary, multi-modal database, in which not only the RGB videos but also the depth maps and skeleton structures of actions of interest are available. Our approach aims to enhance action recognition in RGB videos by leveraging the extra database. Specifically, it optimizes a feature transformation, by which the actions to be recognized can be concisely reconstructed by entries in the auxiliary database. In this way, the inter-database variations are adapted. More importantly, each action can be augmented with additional depth and skeleton images retrieved from the auxiliary database. The proposed approach has been evaluated on three benchmarks of action recognition. The promising results manifest that the augmented depth and skeleton features can lead to remarkable boost in recognition accuracy.}, 
keywords={computer vision;image recognition;Kinect;RGB videos;augmented depth;auxiliary database;complex computer vision tasks;depth associated action recognition;depth images;depth maps;feature transformation;inter-database variations;multimodal database;skeleton associated action recognition;skeleton features;skeleton images;skeleton structures;Cameras;Computer vision;Databases;Kernel;Optimization;Skeleton;Videos}, 
doi={10.1109/CVPR.2014.335},}

@INPROCEEDINGS{7035826, 
author={Barrera, F. and Padoy, N.}, 
booktitle={3D Vision (3DV), 2014 2nd International Conference on}, 
title={Piecewise Planar Decomposition of 3D Point Clouds Obtained from Multiple Static RGB-D Cameras}, 
year={2014}, 
month={Dec}, 
volume={1}, 
pages={194-201}, 
abstract={In this paper, we address the problem of segmenting a 3D point cloud obtained from several RGB-D cameras into a set of 3D piecewise planar regions. This is a fundamental problem in computer vision, whose solution is helpful for further scene analysis, such as support inference and object localisation. In existing planar segmentation approaches for point clouds, the point cloud originates from a single RGB-D view. There is however a growing interest to monitor environments with computer vision setups that contain a set of calibrated 3D cameras located around the scene. To fully exploit the multi-view aspect of such setups, we propose in this paper a novel approach to perform the planar piecewise segmentation directly in 3D. This approach, called Voxel-MRF (V-MRF), is based on discrete 3D Markov random fields, whose nodes correspond to scene voxels and whose labels represent 3D planes. The voxelization of the scene permits to cope with noisy depth measurements, while the MRF formulation provides a natural handling of the 3D spatial constraints during the optimisation. The approach results in a decomposition of the scene into a set of 3D planar patches. A by-product of the method is also a joint planar segmentation of the original images into planar regions with consistent labels across the views. We demonstrate the advantages of our approach using a benchmark dataset of objects with known geometry. We also present qualitative results on challenging data acquired by a multi-camera system installed in two operating rooms.}, 
keywords={Markov processes;cameras;computer vision;image segmentation;3D piecewise planar regions;3D point clouds;3D spatial constraints;Voxel-MRF;computer vision;discrete 3D Markov random fields;joint planar segmentation;multicamera system;multiple static RGB-D cameras;optimisation;piecewise planar decomposition;planar segmentation;scene analysis;Cameras;Geometry;Image segmentation;Joints;Measurement;Object segmentation;Three-dimensional displays;3d-graph-based optimization;RGBD image processing;planar decomposition}, 
doi={10.1109/3DV.2014.57},}

@INPROCEEDINGS{1641019, 
author={Lazebnik, S. and Schmid, C. and Ponce, J.}, 
booktitle={Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on}, 
title={Beyond Bags of Features: Spatial Pyramid Matching for Recognizing Natural Scene Categories}, 
year={2006}, 
month={}, 
volume={2}, 
pages={2169-2178}, 
abstract={This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting "spatial pyramid" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralba’s "gist" and Lowe’s SIFT descriptors.}, 
keywords={Histograms;Image databases;Image recognition;Image representation;Image segmentation;Layout;Object recognition;Robustness;Shape;Spatial databases}, 
doi={10.1109/CVPR.2006.68}, 
ISSN={1063-6919},}

@inproceedings{devilFeatureEncoding,
author={Ken Chatfield and Victor S. Lempitsky and Andrea Vedaldi and Andrew Zisserman},
title={The devil is in the details: an evaluation of recent feature encoding methods},
booktitle={British Machine Vision Conference, {BMVC} 2011, Dundee, UK, August 29 - September 2, 2011. Proceedings},
pages={1--12},
year={2011},
crossref={DBLP:conf/bmvc/2011},
url={http://dx.doi.org/10.5244/C.25.76},
doi={10.5244/C.25.76},
timestamp={Wed, 24 Apr 2013 17:19:07 +0200},
biburl={http://dblp.uni-trier.de/rec/bib/conf/bmvc/ChatfieldLVZ11},
bibsource={dblp computer science bibliography, http://dblp.org},
}

@proceedings{DBLP:conf/bmvc/2011,
  editor    = {Jesse Hoey and
               Stephen J. McKenna and
               Emanuele Trucco},
  title     = {British Machine Vision Conference, {BMVC} 2011, Dundee, UK, August
               29 - September 2, 2011. Proceedings},
  publisher = {{BMVA} Press},
  year      = {2011},
  isbn      = {1-901725-43-X},
  timestamp = {Wed, 24 Apr 2013 17:17:02 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/bmvc/2011},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Choi:2008:SPM:1460096.1460144,
 author = {Choi, Jaesik and Jeon, Won J. and Lee, Sang-Chul},
 title = {Spatio-temporal Pyramid Matching for Sports Videos},
 booktitle = {Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval},
 series = {MIR '08},
 year = {2008},
 isbn = {978-1-60558-312-9},
 location = {Vancouver, British Columbia, Canada},
 pages = {291--297},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1460096.1460144},
 doi = {10.1145/1460096.1460144},
 acmid = {1460144},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {content-based video retrieval, pyramid matching, spatio-temporal, video partitioning},
} 

@INPROCEEDINGS{4587756, 
author={Laptev, I. and Marszalek, M. and Schmid, C. and Rozenfeld, B.}, 
booktitle={Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on}, 
title={Learning realistic human actions from movies}, 
year={2008}, 
month={June}, 
pages={1-8}, 
abstract={The aim of this paper is to address recognition of natural human actions in diverse and realistic video settings. This challenging but important subject has mostly been ignored in the past due to several problems one of which is the lack of realistic and annotated video datasets. Our first contribution is to address this limitation and to investigate the use of movie scripts for automatic annotation of human actions in videos. We evaluate alternative methods for action retrieval from scripts and show benefits of a text-based classifier. Using the retrieved action samples for visual learning, we next turn to the problem of action classification in video. We present a new method for video classification that builds upon and extends several recent ideas including local space-time features, space-time pyramids and multi-channel non-linear SVMs. The method is shown to improve state-of-the-art results on the standard KTH action dataset by achieving 91.8% accuracy. Given the inherent problem of noisy labels in automatic annotation, we particularly investigate and show high tolerance of our method to annotation errors in the training set. We finally apply the method to learning and classifying challenging action classes in movies and show promising results.}, 
keywords={cinematography;image classification;image retrieval;learning (artificial intelligence);support vector machines;video signal processing;automatic video annotation;human action retrieval;local space-time feature;movie script;multichannel nonlinear SVM;space-time pyramid;text-based classifier;video action classification;video realistic human action recognition;visual learning;Cameras;Clothing;Humans;Image recognition;Layout;Motion pictures;Object recognition;Robustness;Text categorization;Video sharing}, 
doi={10.1109/CVPR.2008.4587756}, 
ISSN={1063-6919},}

@INPROCEEDINGS{5995407, 
author={Heng Wang and Klaser, A. and Schmid, C. and Cheng-Lin Liu}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on}, 
title={Action recognition by dense trajectories}, 
year={2011}, 
month={June}, 
pages={3169-3176}, 
abstract={Feature trajectories have shown to be efficient for representing videos. Typically, they are extracted using the KLT tracker or matching SIFT descriptors between frames. However, the quality as well as quantity of these trajectories is often not sufficient. Inspired by the recent success of dense sampling in image classification, we propose an approach to describe videos by dense trajectories. We sample dense points from each frame and track them based on displacement information from a dense optical flow field. Given a state-of-the-art optical flow algorithm, our trajectories are robust to fast irregular motions as well as shot boundaries. Additionally, dense trajectories cover the motion information in videos well. We, also, investigate how to design descriptors to encode the trajectory information. We introduce a novel descriptor based on motion boundary histograms, which is robust to camera motion. This descriptor consistently outperforms other state-of-the-art descriptors, in particular in uncontrolled realistic videos. We evaluate our video description in the context of action classification with a bag-of-features approach. Experimental results show a significant improvement over the state of the art on four datasets of varying difficulty, i.e. KTH, YouTube, Hollywood2 and UCF sports.}, 
keywords={cameras;feature extraction;image classification;image motion analysis;image sampling;image sequences;KLT tracker;SIFT descriptor matching;action recognition;bag-of-features approach;camera motion;dense trajectories;feature extraction;feature trajectories;image classification;motion boundary histograms;optical flow field;uncontrolled realistic videos;Cameras;Feature extraction;Optical imaging;Tracking;Training;Trajectory;Videos}, 
doi={10.1109/CVPR.2011.5995407}, 
ISSN={1063-6919},}

@INPROCEEDINGS{6909493, 
author={Di Wu and Ling Shao}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on}, 
title={Leveraging Hierarchical Parametric Networks for Skeletal Joints Based Action Segmentation and Recognition}, 
year={2014}, 
month={June}, 
pages={724-731}, 
abstract={Over the last few years, with the immense popularity of the Kinect, there has been renewed interest in developing methods for human gesture and action recognition from 3D skeletal data. A number of approaches have been proposed to extract representative features from 3D skeletal data, most commonly hard wired geometric or bio-inspired shape context features. We propose a hierarchial dynamic framework that first extracts high level skeletal joints features and then uses the learned representation for estimating emission probability to infer action sequences. Currently gaussian mixture models are the dominant technique for modeling the emission distribution of hidden Markov models. We show that better action recognition using skeletal features can be achieved by replacing gaussian mixture models by deep neural networks that contain many layers of features to predict probability distributions over states of hidden Markov models. The framework can be easily extended to include a ergodic state to segment and recognize actions simultaneously.}, 
keywords={feature extraction;hidden Markov models;image motion analysis;image recognition;image segmentation;image sequences;Gaussian mixture models;action recognition;action segmentation;action sequences;emission probability estimation;hidden Markov models;hierarchial dynamic framework;hierarchical parametric networks;skeletal joints;skeletal joints feature extraction;Data models;Feature extraction;Hidden Markov models;Joints;Neural networks;Three-dimensional displays;Training}, 
doi={10.1109/CVPR.2014.98},}

@inproceedings{Klaser:2010:HFA:2435051.2435071,
 author = {Klaser, Alexander and Marszalek, Marcin and Schmid, Cordelia and Zisserman, Andrew},
 title = {Human Focused Action Localization in Video},
 booktitle = {Proceedings of the 11th European Conference on Trends and Topics in Computer Vision - Volume Part I},
 series = {ECCV'10},
 year = {2012},
 isbn = {978-3-642-35748-0},
 location = {Heraklion, Crete, Greece},
 pages = {219--233},
 numpages = {15},
 url = {http://dx.doi.org/10.1007/978-3-642-35749-7_17},
 doi = {10.1007/978-3-642-35749-7_17},
 acmid = {2435071},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
 keywords = {HOG, action recognition, human tracking, localization},
}

@INPROCEEDINGS{MSR:5543273, 
author={Wanqing Li and Zhengyou Zhang and Zicheng Liu}, 
booktitle={Computer Vision and Pattern Recognition Workshops (CVPRW), 2010 IEEE Computer Society Conference on}, 
title={Action recognition based on a bag of 3D points}, 
year={2010}, 
month={June}, 
pages={9-14}, 
abstract={This paper presents a method to recognize human actions from sequences of depth maps. Specifically, we employ an action graph to model explicitly the dynamics of the actions and a bag of 3D points to characterize a set of salient postures that correspond to the nodes in the action graph. In addition, we propose a simple, but effective projection based sampling scheme to sample the bag of 3D points from the depth maps. Experimental results have shown that over 90% recognition accuracy were achieved by sampling only about 1% 3D points from the depth maps. Compared to the 2D silhouette based recognition, the recognition errors were halved. In addition, we demonstrate the potential of the bag of points posture model to deal with occlusions through simulation.}, 
keywords={gesture recognition;graph theory;hidden feature removal;motion estimation;3D points;action graph;human action recognition;human motion;occlusions;projection based sampling scheme;Australia;Cameras;Computational modeling;Computer vision;Humans;Joints;Pattern recognition;Sampling methods;Shape;Video sequences}, 
doi={10.1109/CVPRW.2010.5543273}, 
ISSN={2160-7508},}

@INPROCEEDINGS{MSR:6618942, 
author={Oreifej, O. and Zicheng Liu}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on}, 
title={HON4D: Histogram of Oriented 4D Normals for Activity Recognition from Depth Sequences}, 
year={2013}, 
month={June}, 
pages={716-723}, 
abstract={We present a new descriptor for activity recognition from videos acquired by a depth sensor. Previous descriptors mostly compute shape and motion features independently, thus, they often fail to capture the complex joint shape-motion cues at pixel-level. In contrast, we describe the depth sequence using a histogram capturing the distribution of the surface normal orientation in the 4D space of time, depth, and spatial coordinates. To build the histogram, we create 4D projectors, which quantize the 4D space and represent the possible directions for the 4D normal. We initialize the projectors using the vertices of a regular polychoron. Consequently, we refine the projectors using a discriminative density measure, such that additional projectors are induced in the directions where the 4D normals are more dense and discriminative. Through extensive experiments, we demonstrate that our descriptor better captures the joint shape-motion cues in the depth sequence, and thus outperforms the state-of-the-art on all relevant benchmarks.}, 
keywords={computational geometry;image motion analysis;image sensors;image sequences;normal distribution;quantisation (signal);4D normals;4D time space quantization;4Dprojectors;HON4D;activity recognition;depth sensor;depth sequences;discriminative density measurement;histogram-of-oriented 4D normals;joint shape-motion cues;polychoron vertices;spatial coordinates;surface normal orientation distribution;Histograms;Joints;Quantization (signal);Shape;Support vector machines;Three-dimensional displays;Vectors;3D;4D;4D Normals;Action Recognition;Activity Recognition;Depth;HOG;HON;Histogram of Gradients;Histogram of Normals;Kinect;MSR Action 3D;MSR Action Pairs;MSR Daily Activity;Polychoron;Shape}, 
doi={10.1109/CVPR.2013.98}, 
ISSN={1063-6919},}

@inproceedings{MSR:Wang:2012:RAR:2403006.2403071,
 author = {Wang, Jiang and Liu, Zicheng and Chorowski, Jan and Chen, Zhuoyuan and Wu, Ying},
 title = {Robust 3D Action Recognition with Random Occupancy Patterns},
 booktitle = {Proceedings of the 12th European Conference on Computer Vision - Volume Part II},
 series = {ECCV'12},
 year = {2012},
 isbn = {978-3-642-33708-6},
 location = {Florence, Italy},
 pages = {872--885},
 numpages = {14},
 url = {http://dx.doi.org/10.1007/978-3-642-33709-3_62},
 doi = {10.1007/978-3-642-33709-3_62},
 acmid = {2403071},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
} 


@INPROCEEDINGS{MSR:6247813, 
author={Jiang Wang and Zicheng Liu and Ying Wu and Junsong Yuan}, 
booktitle={Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on}, 
title={Mining actionlet ensemble for action recognition with depth cameras}, 
year={2012}, 
month={June}, 
pages={1290-1297}, 
abstract={Human action recognition is an important yet challenging task. The recently developed commodity depth sensors open up new possibilities of dealing with this problem but also present some unique challenges. The depth maps captured by the depth cameras are very noisy and the 3D positions of the tracked joints may be completely wrong if serious occlusions occur, which increases the intra-class variations in the actions. In this paper, an actionlet ensemble model is learnt to represent each action and to capture the intra-class variance. In addition, novel features that are suitable for depth data are proposed. They are robust to noise, invariant to translational and temporal misalignments, and capable of characterizing both the human motion and the human-object interactions. The proposed approach is evaluated on two challenging action recognition datasets captured by commodity depth cameras, and another dataset captured by a MoCap system. The experimental evaluations show that the proposed approach achieves superior performance to the state of the art algorithms.}, 
keywords={cameras;image motion analysis;image recognition;image sensors;object tracking;3D position;MoCap system;actionlet ensemble mining;actionlet ensemble model;commodity depth sensor;depth camera;depth map;human action recognition;human motion;human-object interaction;intraclass variation;occlusion;tracked joints;Cameras;Feature extraction;Hidden Markov models;Humans;Joints;Noise;Robustness}, 
doi={10.1109/CVPR.2012.6247813}, 
ISSN={1063-6919},}

@incollection{RahimKinectOR,
year={2014},
isbn={978-3-319-07520-4},
booktitle={Information Processing in Computer-Assisted Interventions},
volume={8498},
series={Lecture Notes in Computer Science},
editor={Stoyanov, Danail and Collins, D.Louis and Sakuma, Ichiro and Abolmaesumi, Purang and Jannin, Pierre},
doi={10.1007/978-3-319-07521-1_18},
title={Temporally Consistent 3D Pose Estimation in the Interventional Room Using Discrete MRF Optimization over RGBD Sequences},
url={http://dx.doi.org/10.1007/978-3-319-07521-1_18},
publisher={Springer International Publishing},
keywords={Body pose estimation; clinician tracking; surgical workflow analysis; Markov random field; RGBD images},
author={Kadkhodamohammadi, Abdolrahim and Gangi, Afshin and de Mathelin, Michel and Padoy, Nicolas},
pages={168-177},
language={English}
}

@ARTICLE{IXMAS:weinland06,
  author = {Weinland, Daniel and Ronfard, Remi and Boyer, Edmond},
  title = {Free Viewpoint Action Recognition using Motion History Volumes},
  journal = {Computer Vision and Image Understanding},
  year = {2006},
  volume = {104},
  pages = {249--257},
  number = {2-3}
}

@INPROCEEDINGS{IXMAS:weinland10a,
  author = {Daniel Weinland and Mustafa  \"{O}zuysal and Pascal Fua},
  title = {Making Action Recognition Robust to Occlusions and Viewpoint Changes},
  booktitle = {European Conference on Computer Vision},
  year = {2010}
}

@incollection{andruLaparoscopic,
year={2014},
isbn={978-3-319-07520-4},
booktitle={Information Processing in Computer-Assisted Interventions},
volume={8498},
series={Lecture Notes in Computer Science},
editor={Stoyanov, Danail and Collins, D.Louis and Sakuma, Ichiro and Abolmaesumi, Purang and Jannin, Pierre},
doi={10.1007/978-3-319-07521-1_20},
title={Towards Better Laparoscopic Video Database Organization by Automatic Surgery Classification},
url={http://dx.doi.org/10.1007/978-3-319-07521-1_20},
publisher={Springer International Publishing},
keywords={Minimally invasive surgery; laparoscopic video classification; support vector machine; multiple kernel learning},
author={Twinanda, AndruP. and Marescaux, Jacques and De Mathelin, Michel and Padoy, Nicolas},
pages={186-195},
language={English}
}

@inproceedings{Vedaldi:2010:VOP:1873951.1874249,
 author = {Vedaldi, Andrea and Fulkerson, Brian},
 title = {Vlfeat: An Open and Portable Library of Computer Vision Algorithms},
 booktitle = {Proceedings of the International Conference on Multimedia},
 series = {MM '10},
 year = {2010},
 isbn = {978-1-60558-933-6},
 location = {Firenze, Italy},
 pages = {1469--1472},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1873951.1874249},
 doi = {10.1145/1873951.1874249},
 acmid = {1874249},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computer vision, image classification, object recognition, visual features},
} 


@book{criminisi2013decision,
  title={Decision forests for computer vision and medical image analysis},
  author={Criminisi, Antonio and Shotton, Jamie},
  year={2013},
  publisher={Springer Science \& Business Media}
}