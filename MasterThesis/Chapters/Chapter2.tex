% Chapter 2

\chapter{Related Work} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter2} 

\lhead{Chapter 2. \emph{Related Work}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------
%	RELATED WORK
%----------------------------------------------------------------------------------------
% TODO: 
% 	+ BoW approach of related works should be added
%	+ Maybe SPM can be more given in more details

	The problem of activity recognition has been widely studied in the computer vision field. Recent works show excellent performances using local features such as local motions \cite{5995407, 6909503}, human detectors \cite{Klaser:2010:HFA:2435051.2435071} and skeleton tracking \cite{6909731, 6909476, 6909493}. In \cite{6909503}, Yang et al. presented an activity recognition framework for depth sequences. Clustered hypersurface normals are used to construct polynormals to get shape information from depth sequences. The method achieved good results on public Microsoft human action dataset MSR \cite{MSR:5543273, MSR:6618942, MSR:Wang:2012:RAR:2403006.2403071, MSR:6247813}. However, super normal vectors (SNV) encoded method is not reliable either on a highly cluttered and occluded environment or when the actions have very low depth variance.

	More recently, great improvements were obtained by using skeleton tracking based activity recognition approaches. \citet{6909476} proposed a novel skeleton representation that is modeling the 3D geometric relationships between various body parts. They outperformed most of the skeleton-based state-of-the-art methods with their representation. In other work, \citet{6909731} argues that skeleon-based approaches are limited by the effective working distance of the RGBD cameras and practically they are not online all the time. They proposed using the combination of intensity, depth and skeleton structures. In \cite{6909735}, Want et al. stated that action recognition is view-dependent and proposed a novel multiview spatio-temporal graphical representation for cross-view action recognition. The proposed method leverages from 3D human skeleton data. Despite the satisfactory performance of skeleton tracking approaches, frontal camera views of the persons with low occlusion is needed which are not always possible in environments such as the operating rooms. \citet{RahimKinectOR} addressed the problem of skeleton tracking of clinicians in the operating rooms and showed that off-the-shelf skeleton tracking methods fail most of the time.
    
 	In one of the most related work with a multi-view systems and voting scheme, \citet{Zhu201320} addressed the problem of multi-view activity recognition using voting scheme and random forest classifier. They use IXMAS human action dataset \cite{IXMAS:weinland06, IXMAS:weinland10a} which consist of 5 cameras, 13 daily-live actions with various data, e.g., silhouettes, reconstructed volumes. However, the dataset does not consist of real activities, i.e., the dataset is recorded in a laboratory environment. The method is dependent on the extracted 2D silhouettes from each camera. The segments in temporal domain consist of 2D binary silhouettes which are used to train the Random Forest classifier to get prediction histograms used in voting. The voting strategy collects the vote from each segment of video clip and apply the weights on the features and on the camera views. This voting strategy is different than our proposed voting scheme where we divide 4D space into cells to collect votes. Their results showed that combination of multi-view cameras, voting scheme and Random Forest classifier produce good results. On the other hand, in multi-view camera systems, actions may not be available from each camera view all the time. Furthermore, methods like silhouette extraction, human tracking are not feasible solutions to be used in high cluttered and occluded environment, e.g., the crowded places, operating rooms etc.
    
    Similarly, in this thesis, we address the problem of activity recognition in the operating room using a multi-view camera system. It is not a trivial problem because of various challenges. Firstly, camera positioning in operating rooms for better field of view while accommodating the articulation of the surgical equipments is very challenging. In addition, operating rooms are very dense and dynamic with a cluttered background and reflective objects, high occlusions, illumination changes , upfront camera view and very tiny or slow movements. Hence, methods in \cite{6909503, 6909476, 6909731,6909735, Zhu201320} are not sufficient to deal with these problems. Due to these difficulties, it is required to have lower level features to represent the video clips.
    
    One example of the lower level features for an image sequence is spatio-temporal interest point (STIP)\cite{ laptev2005space}. STIPs are sparse interest points along spatial and temporal domain, which are detected by Harris corner detection in 3D spatio-temporal domain. In \cite{1570899}, Dollar et al. proposed a modified sparse and informative STIP and showed that it is robust to pose, clutters and occlusions. The STIPs \cite{1570899} are described with cuboids extracted from surrounding spatio-temporally windowed data. The proposed interest points are more robust to gradual gradient changes and periodic movements. However, the approach was designed to work mainly for intensity data and did work properly on depth data because of the nouse. \citet{6619209} proposed filtering method to detect STIP in depth data, namely depth spatio-temporal interest points (DSTIP). They also introduced a novel feature (i.e., Depth Cuboid Similarity Feature) to describe local similarity of 3D depth cuboids around DSTIP.
    
    In a recent work, \citet{twinanda2015data} showed that intensity or depth data is not sufficient by itself to recognizing some actions in the operating rooms. Some actions are well recognized in depth videos due to illumination problems in intensity data, and some actions give better results in intensity data due to low depth variance of depth data. Hence, the combination of both intensity and depth data have more discriminating power than using only intensity or depth data. The problem of activity recognition in an operating room using multi-view RGBD camera systems was addressed. A dataset of 1734 video clips of 15 surgical activity is generated using 2 RGBD cameras from real surgical operations in a hybrid operating room. Then, STIP \cite{1570899} and DSTIP \cite{6619209} are detected from intensity and depth respectively. They extracted Histogram of Optical Flows (HOF) and Depth Cuboid Similarity Feature (DCSF \cite{6619209}) features around the STIP and DSTIP respectively. These features are encoded by a novel feature encoding scheme that extends the bag-of-words (BoW) approach to learn a non-rigid layout that divides the 4D spatio-temporal space of the feature locations. Achieving 85.53\% accuracy, the work showed that encoding features with the non-rigid layout retains more spatio-temporal information compared to Spatial Pyramid Matching (SPM) \cite{1641019}.


