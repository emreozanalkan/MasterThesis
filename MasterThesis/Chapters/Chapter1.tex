% Chapter 1

\chapter{Introduction} % Main chapter title

\label{Chapter1} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a shortened title

%----------------------------------------------------------------------------------------

Activity recognition using cameras is a very active research topic since the past several decades in the domain of computer vision. It has been widely applied in many fields, such as video surveillance, human computer interaction, robotics and medicine. However, activity recognition is still a very challenging problem due to the large amount of data, amount of computational power required, and high intra-class variations of activities. Over the past decades, researchers have mainly focused on using image sequences from single-view RGB cameras for activity recognition which have many inherent limitations. Single-view camera approaches have limitations due to narrow field of view which makes it more vulnerable to occlusions. Moreover, RGB cameras are very sensitive to illumination changes. 

Thanks to recent technological advances and the emergence of cost-effective depth sensors, depth cameras have become more popular and attracted many researchers to use them in activity recognition studies. Depth sensors have many advantages over RGB cameras. For example, they can provide structural information of the scenes and they can even work in total darkness since infrared structured light is used to reconstruct the scenes. All these advantages make it interesting to incorporate the RGBD cameras into a more challenging environments, e.g., operating rooms (OR).

Recent progresses in medicine transformed operating rooms into hybrid rooms equipped with new imaging devices, robotic arms, sensors and electronic devices. These changes require more management and tracking of information. Therefore, activity recognition in operating rooms has become an important research field in the domain of medicine. It enables applications, such as OR time management for hospitals \cite{Bhatia:2007:RIO:1620113.1620126}, surgical work-flow modeling and monitoring \cite{DBLP:journals/tbe/LalysRBJ12, Padoy2012632}, surgical skill analysis \cite{Zappella2013732} and automatic transcription of medical procedures \cite{6553758}. In particular, activity recognition in operating rooms equipped with x-ray imaging devices can be used to estimate  the radiation exposure of the patients and the clinicians and correlate it to their activities \cite{seeingIsBelievingNL}. 

In order to recognize activities, many data sources from operating rooms can be used, e.g, the vital signs of the patients \cite{Bhatia:2007:RIO:1620113.1620126} and tool usage signals \cite{Padoy2012632}. However, they are only providing local information that is not enough to capture global activities and events in the operating room. Thus, in this thesis, we focus on performing activity recognition on videos. Specifically we are identifying what activities is happening in a video clip.

\citet{twinanda2015data} have addressed the same problem. In their work, they proposed an action recognition pipeline with a novel feature encoding scheme that achieved 85.53\% accuracy. The proposed novel feature encoding extends the bag-of-words (BoW) approach to learn a data-driven non-rigid layout to divide the 4D spatio-temporal (3D spatial + 1D temporal) space of the feature locations. Their work showed that encoding features with the learned non-rigid layout retains more spatio-temporal information compared to the rigid counterpart, i.e, SPM \cite{1641019}. 

In this thesis, we propose to adopt the activity recognition pipeline from \cite{twinanda2015data} and extend it with a voting scheme to solve activity recognition in the operating room. Firstly, spatio-temporal interest points (STIP) and depth spatio-temporal interest points (DSTIP) are detected from intensity and depth video clips respectively. Secondly, histogram of optical flows (HOF) from intensity and depth cuboid similarity features (DCSF \cite{6619209}) from depth data are extracted around the detected interest points. Then we learn two separate dictionaries to encode our extracted features. A visual dictionary is learned to encode visual features: the HOF and the DCSF \cite{6619209}. On the other hand, the spatio-temporal dictionary is learned to define a 4D non-rigid layout. The non-rigid layout divides the 4D spatio-temporal space into smaller 4D patches. From each patch, a histogram of feature points will be counted. Finally, histograms coming from the same video clip will be passed to the classifiers to vote for the action. We also introduce two voting approaches: one-model and multi-model. In the one-model approach, each patch from all video clips are used to train a classifier. On the other hand, the multi-model approach trains a separate classifier for each patch. In order to provide a comprehensive comparison, we present the results for voting sheme and non-voting scheme approaches using non-linear SVM and Random Forest classifiers.

The rest of the thesis is organized as follows. Chapter~\ref{Chapter2} introduces the related work on activity recognition. Chapter~\ref{Chapter3} provides the details of the methodology used in this work. Chapter~\ref{Chapter4} describes and discusses the experimental setup and results. Finally, Chapter~\ref{Chapter5} presents the conclusions of the thesis. 



